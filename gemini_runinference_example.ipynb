{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/Magentic-AlgoTrading101/blob/main/gemini_runinference_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uht8f-mTo040"
      },
      "source": [
        "# Using Gemini on Vertex AI with Apache Beam's RunInference\n",
        "\n",
        "This notebook demonstrates how to build a scalable data processing pipeline that uses a Gemini large language model for inference. We will use **Apache Beam** to create the pipeline and its `RunInference` transform to efficiently call the **Gemini API on Vertex AI**.\n",
        "\n",
        "### Key Components:\n",
        "1.  **`GeminiModelHandler`**: A built-in Beam model handler that connects to the Vertex AI Gemini API.\n",
        "2.  **`RunInference`**: A generic Beam transform that manages batching, parallelism, and execution of model inferences.\n",
        "3.  **`DirectRunner`**: We will use Beam's local runner for this example, but the same code can be executed on a distributed runner like `DataflowRunner` for massive scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPlJY6Xo043"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "First, we need to install the required Python libraries. `apache-beam[gcp]` includes the necessary components for interacting with Google Cloud services, and `google-genai` provides the Gemini API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGLHskYjo044"
      },
      "outputs": [],
      "source": [
        "!pip install apache-beam[gcp]==2.66 -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjeiLyHko045"
      },
      "outputs": [],
      "source": [
        "!pip install google-genai==1.21.1 -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKwEoGsBo045"
      },
      "source": [
        "### Authentication\n",
        "\n",
        "This notebook assumes you are running in an environment where you have already authenticated with Google Cloud. The simplest way to do this for local development is to use the `gcloud` CLI:\n",
        "\n",
        "```bash\n",
        "gcloud auth application-default login\n",
        "```\n",
        "\n",
        "This command makes your user credentials available to libraries like the Vertex AI SDK."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "ZoXrQcVpqUPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFMm_Mzdo046"
      },
      "source": [
        "## 2. Import Libraries\n",
        "\n",
        "Next, we import the necessary classes and functions from Apache Beam and other libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0WXMTW6o046"
      },
      "outputs": [],
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "# Import the core components for ML inference in Beam.\n",
        "from apache_beam.ml.inference.base import RunInference, PredictionResult\n",
        "\n",
        "# Import the specific model handler for Gemini on Vertex AI.\n",
        "from apache_beam.ml.inference.gemini_inference import GeminiModelHandler, generate_from_string\n",
        "\n",
        "# Helper for iterating over collections.\n",
        "from collections.abc import Iterable\n",
        "\n",
        "# Python Package Version\n",
        "from packaging.version import Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BRsRN9no046"
      },
      "source": [
        "## 3. Configuration\n",
        "\n",
        "Set your Google Cloud project details and the model you wish to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGx7cxsVo047"
      },
      "outputs": [],
      "source": [
        "# --- UPDATE THESE VALUES --- #\n",
        "PROJECT_ID = \"your-gcp-project-id\"  # ⬅️ Replace with your Google Cloud project ID\n",
        "LOCATION = \"us-central1\"          # ⬅️ Replace with your desired GCP region\n",
        "\n",
        "# You can choose from available Gemini models in Vertex AI.\n",
        "# For a full list, see: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n",
        "\n",
        "# --- Pipeline Configuration ---\n",
        "# Number of threads to use for the local DirectRunner.\n",
        "NUM_WORKERS = 1\n",
        "\n",
        "# Get the Installed Version of genai Python package\n",
        "\n",
        "#genai_version = Version(genai.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eub5B47Qo047"
      },
      "source": [
        "## 4. Define Pipeline Components\n",
        "\n",
        "### Post-Processor DoFn\n",
        "\n",
        "The `RunInference` transform returns a `PredictionResult` object, which contains both the original input (`example`) and the full, complex response from the API (`inference`). We create a simple Beam `DoFn` (a parallel processing function) to parse this object and extract the clean text output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHxIy3LFo047"
      },
      "outputs": [],
      "source": [
        "class PostProcessor(beam.DoFn):\n",
        "  \"\"\"Parses the PredictionResult to extract a human-readable string.\"\"\"\n",
        "  def process(self, element: PredictionResult) -> Iterable[str]:\n",
        "    \"\"\"\n",
        "    Extracts the generated text from the Gemini API response.\n",
        "\n",
        "    The inference result from GeminiModelHandler is a tuple containing:\n",
        "    ('sdk_http_response', [<google.cloud.aiplatform_v1.types.GenerateContentResponse>])\n",
        "\n",
        "    We navigate this structure to get the final text.\n",
        "    \"\"\"\n",
        "    # The original input prompt is stored in `element.example`\n",
        "    input_prompt = element.example\n",
        "\n",
        "    # The API response is in `element.inference`\n",
        "    # Path to text: response -> candidates -> content -> parts -> text\n",
        "    gemini_response = element.inference[1][0]\n",
        "\n",
        "    # Only supported for genai package 1.21.1 or earlier\n",
        "    output_text = gemini_response.content.parts[0].text\n",
        "\n",
        "    # Yield a formatted string for printing\n",
        "    yield f\"Input:\\n{input_prompt}\\n\\nOutput:\\n{output_text.strip()}\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKv7C8WSo047"
      },
      "source": [
        "### Input Data\n",
        "\n",
        "For this example, we'll use a simple Python list of prompts. In a real-world application, you would replace `beam.Create` with an I/O transform to read from a source like a text file, a database, or a message queue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtHZRd23o048"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"What is 1+2?\",\n",
        "    \"How is the weather in NYC in July?\",\n",
        "    \"Write a short, 3-line poem about a robot learning to paint.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibyvUh37o048"
      },
      "source": [
        "## 5. Define and Run the Beam Pipeline\n",
        "\n",
        "Now we define the main function that constructs and runs the Apache Beam pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3Go5HTvo048"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(prompts, model_name, num_workers):\n",
        "    \"\"\"Constructs and runs the Beam pipeline for Gemini inference.\"\"\"\n",
        "    from google.colab import userdata\n",
        "    # 1. Define the Model Handler\n",
        "    # This object knows how to communicate with the Vertex AI Gemini API.\n",
        "    # `generate_from_string` is a helper that formats a simple string prompt\n",
        "    # into the required API request format.\n",
        "    model_handler = GeminiModelHandler(\n",
        "      model_name=model_name,\n",
        "      request_fn=generate_from_string,\n",
        "      #project=PROJECT_ID,\n",
        "      #location=LOCATION\n",
        "      api_key=userdata.get('GOOGLE_API_KEY')\n",
        "    )\n",
        "\n",
        "    # 2. Set Pipeline Options\n",
        "    # For local execution, we use the DirectRunner.\n",
        "    # `direct_num_workers` controls the number of parallel threads.\n",
        "    pipeline_options = PipelineOptions(\n",
        "        direct_num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # 3. Construct the Pipeline\n",
        "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "        # Create a PCollection from our list of prompts.\n",
        "        read_prompts = pipeline | \"GetPrompts\" >> beam.Create(prompts)\n",
        "\n",
        "        # The core of our pipeline: apply the RunInference transform.\n",
        "        # Beam will handle batching and parallel API calls.\n",
        "        predictions = read_prompts | \"RunInference\" >> RunInference(model_handler)\n",
        "\n",
        "        # Parse the results to get clean text.\n",
        "        processed = predictions | \"PostProcess\" >> beam.ParDo(PostProcessor())\n",
        "\n",
        "        # Print the final, formatted output to the console.\n",
        "        # This is a simple \"sink\" for demonstration purposes.\n",
        "        _ = processed | \"PrintOutput\" >> beam.Map(print)\n",
        "\n",
        "    print(\"\\n--- Pipeline finished ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZN3EYYpo048"
      },
      "source": [
        "## 6. Execute the Pipeline\n",
        "\n",
        "Finally, let's call our function to run the pipeline. The output will be printed directly below this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvfrsE94o048"
      },
      "outputs": [],
      "source": [
        "run_pipeline(prompts, MODEL_NAME, NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKFVYB9fo048"
      },
      "source": [
        "### Expected Output\n",
        "\n",
        "The output should look similar to the following (the exact wording from Gemini may vary slightly):\n",
        "\n",
        "```\n",
        "Input:\n",
        "What is 1+2?\n",
        "\n",
        "Output:\n",
        "3\n",
        "\n",
        "Input:\n",
        "How is the weather in NYC in July?\n",
        "\n",
        "Output:\n",
        "The weather in New York City in July is typically hot and humid. Average high temperatures are around 84°F (29°C), and average low temperatures are around 69°F (21°C). It is also one of the rainiest months of the year, so you can expect some thunderstorms.\n",
        "\n",
        "Input:\n",
        "Write a short, 3-line poem about a robot learning to paint.\n",
        "\n",
        "Output:\n",
        "Steel fingers grip a brush so light,\n",
        "A canvas blooms with colors bright,\n",
        "A circuit hums, a new delight.\n",
        "\n",
        "--- Pipeline finished ---\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}