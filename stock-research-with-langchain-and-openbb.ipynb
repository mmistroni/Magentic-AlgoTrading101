{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### @prompt example","metadata":{}},{"cell_type":"markdown","source":"## LangChain with OpenBB\nThis notebook will leverage the following technologies/frameworks to do some stock research.\nIt will create an Agent that will fetch information for top performing companies in the top performing industry\nIt will then leverage Chroma to load the latest earning transcript call for Altria (MO) to allow users to ask few questions\non Altria's latest result.\nThis work was inspired by this article\n\n- Gemini\n- LangChain\n- OpenBB (openbb.co)","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install -U langchain-google-genai\n!pip install -U -q \"google-genai==1.7.0\"\n!pip install langchain_community\n!pip install docx2txt\n!pip install chromadb\n!pip install wikipedia\n!pip install finvizfinance","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_core.tools import tool\n\n@tool\ndef get_sectors_performance(sector:str = None):\n    \"\"\" Useful for getting performance of each  sector for last week, last month, last quarter, last half year and last year. **This tool does not require any input from the user.**\"\"\"\n    \n    from finvizfinance.group import Performance\n\n    try:\n        performance = Performance()\n        # Get the performance data\n        return performance.screener_view().to_dict('records')\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(\"Please ensure the finvizfinance library is installed correctly.\")\n        print(\"You can install it using: pip install finvizfinance\")\n        print(\"Also, check your internet connection as the library fetches data from Finviz.\")\n\n@tool\ndef get_companies_for_sector(sectorName:str):\n    \"\"\" Return a subset of companies for the given sector\"\"\"\n    from finvizfinance.screener.overview import Overview\n    foverview = Overview()\n    filters_dict = {'Sector': sectorName,\n                    'Market Cap.': '+Small (over $300mln)',\n                    'Average Volume': 'Over 200K',\n                    'Current Ratio': 'Over 1',\n                    'Debt/Equity': 'Under 1',\n                    'InstitutionalOwnership': 'Under 60%',\n                    'Price': 'Over $10'}\n    foverview.set_filter(filters_dict=filters_dict)\n    df = foverview.screener_view(order='Company')\n    return df.head(5).to_dict('records')\n\n\nget_companies_for_sector.invoke('Basic Materials')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.agents import tool\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.prompts import MessagesPlaceholder\nfrom langchain.memory import ConversationTokenBufferMemory\nfrom langchain.agents.format_scratchpad.openai_tools import (\n    format_to_openai_tool_messages,\n)\nfrom langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\n\nfrom langchain.agents import create_structured_chat_agent, AgentExecutor\n\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Chat Memory","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import MessagesPlaceholder\nfrom langchain.memory import ConversationTokenBufferMemory\nfrom langchain.agents.format_scratchpad.openai_tools import (\n    format_to_openai_tool_messages,\n)\nfrom langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\nfrom langchain.agents import AgentExecutor\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.agents import AgentType, initialize_agent\n\nMEMORY_KEY = \"chat_history\"\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are very powerful stock recommendation assistant , but dont know current events so you should use your tools as much as you can.\",\n        ),\n        MessagesPlaceholder(variable_name=MEMORY_KEY),\n        (\"user\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.agents import AgentType, Tool, initialize_agent\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\n\n# Initialize Gemini Pro model\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.0-flash\",\n    google_api_key=GOOGLE_API_KEY,\n    temperature=0.7\n)\n\n\n# Define the tools and create a \"tools\" node.\n\ntools = [get_sectors_performance, get_companies_for_sector ]\n\n# Attach the tools to the model so that it knows what it can call.\n#llm_with_tools = llm.bind_tools(tools)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chat_history = []\nchat_history.append(HumanMessage(content=\"Your question here\"))\nchat_history.append(AIMessage(content=\"AI response here\"))\n# Define your prompt\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, max_token_limit=18000)\n\n# Initialize the agent\nagent_chain = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finding the best performing sector across week, month, quarter, and year","metadata":{}},{"cell_type":"code","source":"input1 = '''Find the sector which across week, month, quarter and year has shown the best performance and summarize it.\n'''\nresult = agent_chain.invoke({\"input\": input1, \"chat_history\": chat_history})\nchat_history.extend(\n    [\n        HumanMessage(content=input1),\n        AIMessage(content=result[\"output\"]),\n    ]\n)\nprint(result['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Now we are going to find few companies for the best performing sector. We'll output a table sorted by P/E","metadata":{}},{"cell_type":"code","source":"input1 = \"Now find me some companies for the sector you found in the previous step. Create  a table with Ticker, Company,  P/E and change\"\nresult = agent_chain.invoke({\"input\": input1, \"chat_history\": chat_history})\nprint(result['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Now we will play around with Chroma\n### We will upload an extract from Altria Group latest Earning calls and ask our llm to extract some information","metadata":{}},{"cell_type":"code","source":"!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T19:38:42.202508Z","iopub.execute_input":"2025-04-11T19:38:42.202908Z","iopub.status.idle":"2025-04-11T19:38:47.643025Z","shell.execute_reply.started":"2025-04-11T19:38:42.202879Z","shell.execute_reply":"2025-04-11T19:38:47.641334Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"from langchain.agents import initialize_agent, AgentType\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.tools import Tool\nfrom langchain.vectorstores import Chroma\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nimport os\n\n# --- Configure Google API Key (if needed) ---\n# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n\n# --- Define the LLM and Embeddings ---\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n\n# --- Load and Index the Text Document ---\ndef load_document_into_chroma(file_path, persist_directory=\"chroma_db_single\"):\n    \"\"\"Loads a text document, creates embeddings, and stores it in ChromaDB.\"\"\"\n    loader = TextLoader(file_path)\n    documents = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n    chunks = text_splitter.split_documents(documents)\n    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n    vectorstore.persist()\n    return vectorstore\n\n# --- Define the Retrieval Tool ---\ndef query_chroma_document(query):\n    \"\"\"Queries the ChromaDB for the loaded document.\"\"\"\n    vectorstore = Chroma(persist_directory=\"chroma_db_single\", embedding_function=embeddings)\n    retriever = vectorstore.as_retriever()\n    results = retriever.get_relevant_documents(query)\n    return \"\\n\\n\".join([doc.page_content for doc in results])\n\nvectorstore = load_document_into_chroma('/kaggle/input/altria-q424-earning-call/Altria_Q424EarningCall.txt')\n\nretriever = vectorstore.as_retriever(search_type='mmr', search_kwargs = {'k' : 3, 'lambda_mult' : 0.7})\nretrieval_tool = create_retriever_tool(retriever= retriever,\n                                        name=\"Altria Q424 Earning calls\",\n                                        description=\"For any questions regarding Altria latest earning calls, you must use this tool\")\n\n\n# --- Define the Prompt Template for the Agent ---\nMEMORY_KEY = \"chat_history\"\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful assistant that can retrieve information from a specific document based on user queries.\",\n        ),\n        MessagesPlaceholder(variable_name=MEMORY_KEY),\n        (\"user\", \"{input}\"),\n        (\"user\", \"Use the 'Document Retriever' tool to find relevant information in the document.\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)\n\n# --- Initialize Memory for the Agent ---\nmemory = ConversationBufferMemory(memory_key=MEMORY_KEY, return_messages=True)\n\n# --- Initialize the Agent ---\nchromaagent = initialize_agent(\n    [retrieval_tool],\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True,\n    memory=memory,\n    prompt=prompt,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T20:01:13.076511Z","iopub.execute_input":"2025-04-11T20:01:13.076843Z","iopub.status.idle":"2025-04-11T20:01:14.360211Z","shell.execute_reply.started":"2025-04-11T20:01:13.076816Z","shell.execute_reply":"2025-04-11T20:01:14.358209Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-91-9fb2e3c04ae3>:28: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n  vectorstore.persist()\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-9fb2e3c04ae3>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mmr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'k'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambda_mult'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m retrieval_tool = create_retriever_tool(retriever= retriever,\n\u001b[0m\u001b[1;32m     43\u001b[0m                                         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Altria Q424 Earning calls\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                         description=\"For any questions regarding Altria latest earning calls, you must use this tool\")\n","\u001b[0;31mNameError\u001b[0m: name 'create_retriever_tool' is not defined"],"ename":"NameError","evalue":"name 'create_retriever_tool' is not defined","output_type":"error"}],"execution_count":91},{"cell_type":"markdown","source":"### Now we'll query for risk factors and opportunities from earning calls","metadata":{}},{"cell_type":"code","source":"input1 = '''From the Altria Earning call, please extract the risk factors for the company and summarize CEO commentaryind the sector which across week, month, quarter and year has shown the best performance and summarize it.'''\nresult = chroma_agent.invoke({\"input\": input1, \"chat_history\": chat_history})\nprint(result['output'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T19:49:19.164693Z","iopub.execute_input":"2025-04-11T19:49:19.165031Z","iopub.status.idle":"2025-04-11T19:49:21.784833Z","shell.execute_reply.started":"2025-04-11T19:49:19.164999Z","shell.execute_reply":"2025-04-11T19:49:21.783035Z"}},"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3mOkay, I need to extract the risk factors for Altria and summarize the CEO's commentary on the best-performing sector across different timeframes (week, month, quarter, year).\n\nFirst, I'll use the document retriever to find the risk factors.\nAction: Document Retriever\nAction Input: \"Altria risk factors\"\u001b[0m\nObservation: \u001b[36;1m\u001b[1;3m\u001b[0m\nThought:\u001b[32;1m\u001b[1;3mI need the content of the earning call to answer the question. I will try to retrieve the entire document.\nAction: Document Retriever\nAction Input: \"Altria Earning call\"\u001b[0m\nObservation: \u001b[36;1m\u001b[1;3m\u001b[0m\nThought:","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0;31m# Call the LLM to see what to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             output = self._action_agent.plan(\n\u001b[0m\u001b[1;32m   1359\u001b[0m                 \u001b[0mintermediate_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36mplan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0mfull_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/mrkl/output_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"Action\\s*\\d*\\s*:[\\s]*(.*?)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOTALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             raise OutputParserException(\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0;34mf\"Could not parse LLM output: `{text}`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `I apologize, but I am unable to answer your question. I do not have access to the content of the Altria Earning call. Therefore, I cannot extract the risk factors or summarize the CEO's commentary.`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-90-bc338912e6c0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'''From the Altria Earning call, please extract the risk factors for the company and summarize CEO commentaryind the sector which across week, month, quarter and year has shown the best performance and summarize it.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchroma_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chat_history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m   1625\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001b[1;32m   1329\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1330\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1331\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 for a in self._iter_next_step(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001b[1;32m   1329\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1330\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1331\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 for a in self._iter_next_step(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 \u001b[0mraise_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1370\u001b[0m                     \u001b[0;34m\"An output parsing error occurred. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                     \u001b[0;34m\"In order to pass this error back to the agent and have it try \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I apologize, but I am unable to answer your question. I do not have access to the content of the Altria Earning call. Therefore, I cannot extract the risk factors or summarize the CEO's commentary.`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "],"ename":"ValueError","evalue":"An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I apologize, but I am unable to answer your question. I do not have access to the content of the Altria Earning call. Therefore, I cannot extract the risk factors or summarize the CEO's commentary.`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ","output_type":"error"}],"execution_count":90},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}